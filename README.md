
# Assignment 23

# Problem Statement

To train and understand CLIP for image to text and text to image

Contrastive Language-Image Pre-training (CLIP) is a multimodal learning architecture developed by OpenAI. 
It learns visual concepts from natural language supervision.

ClIP uses two separate architectures as the backbone for encoding vision and text datasets:

- image_encoder: Represents the neural network architecture (e.g., ResNet or Vision Transformer) responsible for encoding images.
- text_encoder: Represents the neural network architecture (e.g., CBOW, BERT, or Text Transformer) responsible for encoding textual information.


## References

1. https://viso.ai/deep-learning/clip-machine-learning/
2. https://towardsdatascience.com/simple-implementation-of-openai-clip-model-a-tutorial-ace6ff01d9f2

This assignment is for undestanding the working and architecture of CLIP model. The implementation is done in hugging spaces in link below:
https://huggingface.co/spaces/Vvaann/Assignment_23/tree/main
